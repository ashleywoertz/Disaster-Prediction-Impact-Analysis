{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load data + clean\n",
        "df = pd.read_excel('/content/public_emdat_custom_request_2025-08-30_c8186e8d-2df5-46a3-b58a-206f64fe2dbc.xlsx')\n",
        "df.columns = df.columns.str.strip()\n",
        "df = df[df['Disaster Group'] == 'Natural']\n",
        "df = df.dropna(subset=['Total Deaths', 'Start Year'])\n",
        "\n",
        "# fill in missing fields\n",
        "for col in ['Magnitude', 'Total Affected', 'No. Injured', 'No. Homeless']:\n",
        "    df[col] = df[col].fillna(0)\n",
        "\n",
        "features = ['Disaster Type', 'Magnitude', 'Region', 'Total Affected', 'No. Injured', 'No. Homeless']\n",
        "X = pd.get_dummies(df[features], drop_first=True)\n",
        "y = df['Total Deaths']\n",
        "\n",
        "# train ML model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# base predictions\n",
        "df['Pred_Historic'] = model.predict(X)\n",
        "\n",
        "# Disaster mortality reduction factors based on studies\n",
        "# Cyclone: 67% reduction\n",
        "#   Source: China's earthquake early warning system led to significant casualty reductions\n",
        "#   (Wikipedia: https://en.wikipedia.org/wiki/ICL_Earthquake_Early_Warning_System)\n",
        "\n",
        "# Flood: 30% reduction\n",
        "#   Source: Global Commission on Adaptation report: 24-hour advance warnings reduce damage/mortality\n",
        "#   (CFR: https://www.cfr.org/expert-brief/cuts-early-warning-systems-are-leaving-us-unprepared-summer-floods)\n",
        "\n",
        "# Storm: 30% reduction\n",
        "#   Source: UNDRR findings: multi-hazard early warning systems reduce mortality significantly\n",
        "#   (WMO: https://wmo.int/publication-series/global-status-of-multi-hazard-early-warning-systems-2024)\n",
        "\n",
        "# Heat wave: 30% reduction\n",
        "#   Source: Studies indicate early warnings reduce heatwave-related mortality by ~30%\n",
        "#   (e.g., UN climate adaptation literature)\n",
        "\n",
        "# Earthquake: 50% reduction\n",
        "#   Source: Short-lead earthquake early warnings in China reduced casualties substantially\n",
        "#   (Wikipedia: https://en.wikipedia.org/wiki/ICL_Earthquake_Early_Warning_System)\n",
        "\n",
        "reduction_lookup = {\n",
        "    'Cyclone': 0.67,\n",
        "    'Flood': 0.30,\n",
        "    'Storm': 0.30,\n",
        "    'Heat wave': 0.30,\n",
        "    'Earthquake': 0.50\n",
        "}\n",
        "\n",
        "df['Reduction'] = df['Disaster Type'].map(reduction_lookup).fillna(0.30)\n",
        "df['AI_Pred'] = df['Pred_Historic'] * (1 - df['Reduction'])\n",
        "\n",
        "# aggregate\n",
        "agg = df.groupby('Start Year').agg({\n",
        "    'Total Deaths': 'sum',\n",
        "    'AI_Pred': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# create chart\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(agg['Start Year'], agg['Total Deaths'], label='Actual Deaths', color='red')\n",
        "plt.plot(agg['Start Year'], agg['AI_Pred'], label='AI Early-Warning Scenario', color='blue', linestyle='--')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Total Deaths')\n",
        "plt.title('Projected Lives Saved from Natural Disasters with AI Early-Warning Systems')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# total & average deaths\n",
        "total_actual = agg['Total Deaths'].sum()\n",
        "total_ai = agg['AI_Pred'].sum()\n",
        "avg_actual = agg['Total Deaths'].mean()\n",
        "avg_ai = agg['AI_Pred'].mean()\n",
        "max_saved_year = (agg['Total Deaths'] - agg['AI_Pred']).idxmax()\n",
        "year_max_saved = agg.loc[max_saved_year, 'Start Year']\n",
        "max_saved = agg.loc[max_saved_year, 'Total Deaths'] - agg.loc[max_saved_year, 'AI_Pred']\n",
        "\n",
        "print(f\"Total deaths (actual): {total_actual:,.0f}\")\n",
        "print(f\"Total deaths (AI scenario): {total_ai:,.0f}\")\n",
        "print(f\"Average annual deaths (actual): {avg_actual:,.0f}\")\n",
        "print(f\"Average annual deaths (AI scenario): {avg_ai:,.0f}\")\n",
        "print(f\"Year with most lives saved: {int(year_max_saved)} ({int(max_saved):,.0f} deaths prevented)\")"
      ],
      "metadata": {
        "id": "jeK1WTCCwCiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def load_data_with_fallback():\n",
        "    \"\"\"Load data files\"\"\"\n",
        "\n",
        "    # local file paths\n",
        "    zip_shapefile = '/content/tl_2022_us_zcta520.shp'\n",
        "    zip_premiums_path = '/content/Supporting_Underlying_Metrics_and_Disclaimer_for_Analyses_of_US_Homeowners_Insurance_Markets_2018-2022.xlsx'\n",
        "    nri_csv = '/content/NRI_Table_Counties.csv'\n",
        "    zip_to_county_path = '/content/ZIP-COUNTY-FIPS_2017-06.csv'\n",
        "\n",
        "    # try to load geo shapefile\n",
        "    gdf_zip = None\n",
        "    try:\n",
        "        if Path(zip_shapefile).exists():\n",
        "            gdf_zip = gpd.read_file(zip_shapefile, columns=['ZCTA5CE20', 'geometry'])\n",
        "            gdf_zip['ZIP'] = gdf_zip['ZCTA5CE20'].astype(str).str.zfill(5)\n",
        "            # reduce size = lower run time\n",
        "            gdf_zip['geometry'] = gdf_zip['geometry'].simplify(0.05)\n",
        "        else:\n",
        "            print(\"Shapefile not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading shapefile: {e}\")\n",
        "        gdf_zip = None\n",
        "\n",
        "    # try to load data\n",
        "    try:\n",
        "        if Path(zip_premiums_path).exists():\n",
        "            df_zip = pd.read_excel(zip_premiums_path, sheet_name=2, dtype={'ZIP Code': str})\n",
        "            df_zip.columns = df_zip.columns.str.strip()\n",
        "            df_zip = df_zip[['ZIP Code', 'Premiums Per Policy']]\n",
        "            df_zip['ZIP Code'] = df_zip['ZIP Code'].str.zfill(5)\n",
        "            # remove any invalid/missing ZIPs\n",
        "            df_zip = df_zip.dropna(subset=['ZIP Code', 'Premiums Per Policy'])\n",
        "            print(f\"Loaded data for {len(df_zip)} ZIP codes\")\n",
        "        else:\n",
        "            print(\"Data file not found\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "    # try to load NRI data\n",
        "    try:\n",
        "        if Path(nri_csv).exists():\n",
        "            # loads all of the cols\n",
        "            df_nri_full = pd.read_csv(nri_csv, dtype=str, nrows=5)\n",
        "            print(f\"Available NRI columns: {list(df_nri_full.columns)}\")\n",
        "            # name sometimes changes so to capture all\n",
        "            possible_fips_cols = ['STCOFIPS', 'COUNTYFIPS', 'FIPS', 'CountyFIPS']\n",
        "            possible_risk_cols = ['RISK_SCORE', 'RISK_VALUE', 'RISKVALUE', 'RISK_RATNG', 'RISK_RATING']\n",
        "\n",
        "            fips_col = None\n",
        "            risk_col = None\n",
        "\n",
        "            for col in possible_fips_cols:\n",
        "                if col in df_nri_full.columns:\n",
        "                    fips_col = col\n",
        "                    break\n",
        "\n",
        "            for col in possible_risk_cols:\n",
        "                if col in df_nri_full.columns:\n",
        "                    risk_col = col\n",
        "                    break\n",
        "\n",
        "            if fips_col is None or risk_col is None:\n",
        "                return None\n",
        "\n",
        "            df_nri = pd.read_csv(nri_csv, dtype=str, usecols=[fips_col, risk_col])\n",
        "            df_nri = df_nri.rename(columns={fips_col: 'COUNTYFIPS', risk_col: 'historical_risk'})\n",
        "            # STCOFIPS column = full 5-digit FIPS code (state + county)\n",
        "            df_nri['COUNTYFIPS'] = df_nri['COUNTYFIPS'].str.zfill(5)\n",
        "            df_nri['historical_risk'] = pd.to_numeric(df_nri['historical_risk'], errors='coerce')\n",
        "            # remove rows w/ missing risk values\n",
        "            df_nri = df_nri.dropna(subset=['historical_risk'])\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "    # load crosswalk data\n",
        "    try:\n",
        "        if Path(zip_to_county_path).exists():\n",
        "            df_sample = pd.read_csv(zip_to_county_path, dtype=str, nrows=5)\n",
        "            possible_zip_cols = ['ZIP', 'ZCTA', 'ZIPCODE']\n",
        "            possible_county_cols = ['STCOUNTYFP', 'COUNTY_FIPS', 'COUNTYFIPS']\n",
        "\n",
        "            zip_col = None\n",
        "            county_col = None\n",
        "\n",
        "            for col in possible_zip_cols:\n",
        "                if col in df_sample.columns:\n",
        "                    zip_col = col\n",
        "                    break\n",
        "\n",
        "            for col in possible_county_cols:\n",
        "                if col in df_sample.columns:\n",
        "                    county_col = col\n",
        "                    break\n",
        "\n",
        "            if zip_col is None or county_col is None:\n",
        "                return None\n",
        "\n",
        "            df_zip_county = pd.read_csv(zip_to_county_path, dtype=str, usecols=[zip_col, county_col])\n",
        "            df_zip_county = df_zip_county.rename(columns={zip_col: 'ZIP', county_col: 'COUNTY_FIPS'})\n",
        "            df_zip_county['ZIP'] = df_zip_county['ZIP'].str.zfill(5)\n",
        "            df_zip_county['COUNTY_FIPS'] = df_zip_county['COUNTY_FIPS'].str.zfill(5)\n",
        "            # remove duplicates + missing vals\n",
        "            df_zip_county = df_zip_county.dropna().drop_duplicates(subset=['ZIP'])\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "    return gdf_zip, df_zip, df_nri, df_zip_county\n",
        "\n",
        "def main():\n",
        "    \"\"\"main function to run the analysis\"\"\"\n",
        "    # load data\n",
        "    gdf_zip, df_zip, df_nri, df_zip_county = load_data_with_fallback()\n",
        "\n",
        "    # DEBUG\n",
        "    print(f\"ZIP codes in data: {len(df_zip)}\")\n",
        "    print(f\"ZIP codes in crosswalk: {len(df_zip_county)}\")\n",
        "    print(f\"Counties in NRI: {len(df_nri)}\")\n",
        "\n",
        "    # check for ZIP code overlap\n",
        "    common_zips = set(df_zip['ZIP Code']).intersection(set(df_zip_county['ZIP']))\n",
        "\n",
        "    # check for county FIPS overlap\n",
        "    common_counties = set(df_zip_county['COUNTY_FIPS']).intersection(set(df_nri['COUNTYFIPS']))\n",
        "\n",
        "    # merge premiums data -> crosswalk -> risk\n",
        "    df_zip = df_zip.merge(df_zip_county, left_on='ZIP Code', right_on='ZIP', how='inner')\n",
        "    df_zip = df_zip.merge(df_nri, left_on='COUNTY_FIPS', right_on='COUNTYFIPS', how='inner')\n",
        "\n",
        "    # merge to GeoDataFrame\n",
        "    gdf_zip = gdf_zip.merge(\n",
        "        df_zip[['ZIP Code','Premiums Per Policy','historical_risk']],\n",
        "        left_on='ZIP', right_on='ZIP Code',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # remove rows with missing data\n",
        "    gdf_viz = gdf_zip.dropna(subset=['Premiums Per Policy', 'historical_risk']).copy()\n",
        "\n",
        "    if len(gdf_viz) == 0:\n",
        "        print(\"No valid data.\")\n",
        "        return\n",
        "\n",
        "    # calc premium changes\n",
        "    print(\"\\nCalculating premium changes...\")\n",
        "    print(f\"Historical risk range: {gdf_viz['historical_risk'].min():.2f} to {gdf_viz['historical_risk'].max():.2f}\")\n",
        "    print(f\"Historical risk mean: {gdf_viz['historical_risk'].mean():.2f}\")\n",
        "    print(f\"Historical risk std: {gdf_viz['historical_risk'].std():.2f}\")\n",
        "\n",
        "    risk_max = gdf_viz['historical_risk'].max()\n",
        "\n",
        "    if risk_max <= 100:\n",
        "        # using RISK_SCORE (0-100 scale)\n",
        "        # Map risk score to premium changes\n",
        "        # 0-20: -10% to -5% (low risk areas get discounts)\n",
        "        # 20-40: -5% to 0% (below average risk)\n",
        "        # 40-60: 0% to +5% (average risk)\n",
        "        # 60-80: +5% to +15% (above average risk)\n",
        "        # 80-100: +15% to +20% (high risk areas)\n",
        "\n",
        "        risk_score = gdf_viz['historical_risk']\n",
        "        gdf_viz['premium_change_pct'] = np.where(\n",
        "            risk_score <= 20, -10 + (risk_score / 20) * 5,  # -10% to -5%\n",
        "            np.where(\n",
        "                risk_score <= 40, -5 + ((risk_score - 20) / 20) * 5,  # -5% to 0%\n",
        "                np.where(\n",
        "                    risk_score <= 60, 0 + ((risk_score - 40) / 20) * 5,  # 0% to +5%\n",
        "                    np.where(\n",
        "                        risk_score <= 80, 5 + ((risk_score - 60) / 20) * 10,  # +5% to +15%\n",
        "                        15 + ((risk_score - 80) / 20) * 5  # +15% to +20%\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        # using RISK_VALUE (dollar amounts)\n",
        "        # normalize w/ z-score for better variation\n",
        "        risk_mean = gdf_viz['historical_risk'].mean()\n",
        "        risk_std = gdf_viz['historical_risk'].std()\n",
        "        gdf_viz['risk_zscore'] = (gdf_viz['historical_risk'] - risk_mean) / risk_std\n",
        "\n",
        "        # map z-scores to premium changes\n",
        "        # Z-score of -2 or less: -10% change\n",
        "        # Z-score of -1: -5% change\n",
        "        # Z-score of 0 (mean): 0% change\n",
        "        # Z-score of +1: +10% change\n",
        "        # Z-score of +2 or more: +20% change\n",
        "\n",
        "        gdf_viz['premium_change_pct'] = gdf_viz['risk_zscore'] * 10  # 10% per standard deviation\n",
        "\n",
        "    # applying realistic bounds\n",
        "    gdf_viz['premium_change_pct'] = np.clip(gdf_viz['premium_change_pct'], -10, 20)\n",
        "\n",
        "    print(f\"Premium change range: {gdf_viz['premium_change_pct'].min():.1f}% to {gdf_viz['premium_change_pct'].max():.1f}%\")\n",
        "    print(f\"Premium change mean: {gdf_viz['premium_change_pct'].mean():.1f}%\")\n",
        "    print(f\"Premium change std: {gdf_viz['premium_change_pct'].std():.1f}%\")\n",
        "\n",
        "    # print distribution\n",
        "    for pct in [10, 25, 50, 75, 90]:\n",
        "        val = np.percentile(gdf_viz['premium_change_pct'], pct)\n",
        "        print(f\"{pct}th percentile: {val:.1f}%\")\n",
        "\n",
        "    gdf_viz['premium_2025'] = gdf_viz['Premiums Per Policy'] * (\n",
        "        1 + gdf_viz['premium_change_pct'] / 100\n",
        "    )\n",
        "\n",
        "    if gdf_viz.crs != 'EPSG:3857':\n",
        "        gdf_viz = gdf_viz.to_crs(\"EPSG:3857\")\n",
        "\n",
        "    # create plot\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
        "    gdf_viz.plot(\n",
        "        column='premium_change_pct',\n",
        "        cmap='coolwarm',\n",
        "        legend=True,\n",
        "        legend_kwds={\n",
        "            'label': \"Projected % Change in Insurance Premiums\",\n",
        "            'orientation': \"horizontal\",\n",
        "            'shrink': 0.8\n",
        "        },\n",
        "        vmin=-10,\n",
        "        vmax=20,\n",
        "        ax=ax,\n",
        "        edgecolor='black',\n",
        "        linewidth=0.1\n",
        "    )\n",
        "\n",
        "    ax.set_title('Projected ZIP-Level Insurance Premium Changes with NRI Risk (2025)',\n",
        "                fontsize=16, pad=20)\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    # stats\n",
        "    stats_text = f\"\"\"\n",
        "    Data Summary:\n",
        "    • ZIP codes analyzed: {len(gdf_viz):,}\n",
        "    • Avg premium change: {gdf_viz['premium_change_pct'].mean():.1f}%\n",
        "    • Min change: {gdf_viz['premium_change_pct'].min():.1f}%\n",
        "    • Max change: {gdf_viz['premium_change_pct'].max():.1f}%\n",
        "    \"\"\"\n",
        "\n",
        "    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=10,\n",
        "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Analysis complete!\")\n",
        "\n",
        "    # print summary stats\n",
        "    print(f\"\\nSummary Statistics:\")\n",
        "    print(f\"Average premium change: {gdf_viz['premium_change_pct'].mean():.2f}%\")\n",
        "    print(f\"Median premium change: {gdf_viz['premium_change_pct'].median():.2f}%\")\n",
        "    print(f\"Standard deviation: {gdf_viz['premium_change_pct'].std():.2f}%\")\n",
        "\n",
        "    return gdf_viz\n",
        "\n",
        "# run the actual analysis\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        result_gdf = main()\n",
        "    except Exception as e:\n",
        "        print(f\"Error running analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "6hx_kGNC9ifL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def load_housing_data_with_fallback():\n",
        "    \"\"\"Load data files with fallback to sample data\"\"\"\n",
        "\n",
        "    # file paths\n",
        "    housing_data_path = '/content/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv'\n",
        "    nri_csv = '/content/NRI_Table_Counties.csv'\n",
        "    zip_to_county_path = '/content/ZIP-COUNTY-FIPS_2017-06.csv'\n",
        "\n",
        "    # load Zillow data\n",
        "    try:\n",
        "        if Path(housing_data_path).exists():\n",
        "            print(\"Loading Zillow ZHVI housing data...\")\n",
        "            df_housing_full = pd.read_csv(housing_data_path, dtype={'RegionName': str})\n",
        "\n",
        "            # get most recent month's data\n",
        "            # YYYY-MM-DD format\n",
        "            date_columns = []\n",
        "            for col in df_housing_full.columns:\n",
        "                if col not in ['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', 'State', 'City', 'Metro', 'CountyName']:\n",
        "                    try:\n",
        "                        pd.to_datetime(col)\n",
        "                        date_columns.append(col)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            if not date_columns:\n",
        "                return None\n",
        "\n",
        "            # sort for most recent\n",
        "            date_columns.sort()\n",
        "            most_recent_date = date_columns[-1]\n",
        "            print(f\"Using most recent data from: {most_recent_date}\")\n",
        "\n",
        "            # get cols\n",
        "            df_housing = df_housing_full[['RegionName', 'StateName', 'City', 'CountyName', most_recent_date]].copy()\n",
        "\n",
        "            # cleaning!!\n",
        "            df_housing['RegionName'] = df_housing['RegionName'].astype(str).str.zfill(5)\n",
        "            df_housing = df_housing.rename(columns={\n",
        "                most_recent_date: 'Current_Value',\n",
        "                'RegionName': 'ZIP_Code'\n",
        "            })\n",
        "\n",
        "            # remove rows w/ missing home vals\n",
        "            df_housing = df_housing.dropna(subset=['Current_Value'])\n",
        "            df_housing = df_housing[df_housing['Current_Value'] > 0]\n",
        "\n",
        "            # location string\n",
        "            df_housing['Location'] = df_housing['City'] + ', ' + df_housing['StateName']\n",
        "\n",
        "            print(f\"Loaded Zillow ZHVI data for {len(df_housing)} ZIP codes\")\n",
        "            print(f\"Value range: ${df_housing['Current_Value'].min():,.0f} to ${df_housing['Current_Value'].max():,.0f}\")\n",
        "            print(f\"Sample ZIP codes: {df_housing['ZIP_Code'].head().tolist()}\")\n",
        "\n",
        "        else:\n",
        "            print(\"Zillow ZHVI data file not found\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Zillow housing data: {e}\")\n",
        "        return None\n",
        "\n",
        "    # try to load NRI data\n",
        "    try:\n",
        "        if Path(nri_csv).exists():\n",
        "            df_nri_full = pd.read_csv(nri_csv, dtype=str, nrows=5)\n",
        "            possible_fips_cols = ['STCOFIPS', 'COUNTYFIPS', 'FIPS', 'CountyFIPS']\n",
        "            possible_risk_cols = ['RISK_SCORE', 'RISK_VALUE', 'RISKVALUE', 'RISK_RATNG', 'RISK_RATING']\n",
        "\n",
        "            fips_col = None\n",
        "            risk_col = None\n",
        "\n",
        "            for col in possible_fips_cols:\n",
        "                if col in df_nri_full.columns:\n",
        "                    fips_col = col\n",
        "                    break\n",
        "\n",
        "            for col in possible_risk_cols:\n",
        "                if col in df_nri_full.columns:\n",
        "                    risk_col = col\n",
        "                    break\n",
        "\n",
        "            if fips_col is None or risk_col is None:\n",
        "                print(f\"could not find required columns\")\n",
        "                return None\n",
        "\n",
        "            df_nri = pd.read_csv(nri_csv, dtype=str, usecols=[fips_col, risk_col])\n",
        "            df_nri = df_nri.rename(columns={fips_col: 'COUNTYFIPS', risk_col: 'climate_risk'})\n",
        "            df_nri['COUNTYFIPS'] = df_nri['COUNTYFIPS'].str.zfill(5)\n",
        "            df_nri['climate_risk'] = pd.to_numeric(df_nri['climate_risk'], errors='coerce')\n",
        "            df_nri = df_nri.dropna(subset=['climate_risk'])\n",
        "            print(f\"Loaded NRI data for {len(df_nri)} counties\")\n",
        "        else:\n",
        "            print(\"NRI data file not found\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading NRI data: {e}\")\n",
        "        return None\n",
        "\n",
        "    # try to load crosswalk data\n",
        "    try:\n",
        "        if Path(zip_to_county_path).exists():\n",
        "            df_sample = pd.read_csv(zip_to_county_path, dtype=str, nrows=5)\n",
        "            possible_zip_cols = ['ZIP', 'ZCTA', 'ZIPCODE']\n",
        "            possible_county_cols = ['STCOUNTYFP', 'COUNTY_FIPS', 'COUNTYFIPS']\n",
        "            zip_col = None\n",
        "            county_col = None\n",
        "\n",
        "            for col in possible_zip_cols:\n",
        "                if col in df_sample.columns:\n",
        "                    zip_col = col\n",
        "                    break\n",
        "\n",
        "            for col in possible_county_cols:\n",
        "                if col in df_sample.columns:\n",
        "                    county_col = col\n",
        "                    break\n",
        "\n",
        "            if zip_col is None or county_col is None:\n",
        "                return None\n",
        "\n",
        "            df_zip_county = pd.read_csv(zip_to_county_path, dtype=str, usecols=[zip_col, county_col])\n",
        "            df_zip_county = df_zip_county.rename(columns={zip_col: 'ZIP', county_col: 'COUNTY_FIPS'})\n",
        "            df_zip_county['ZIP'] = df_zip_county['ZIP'].str.zfill(5)\n",
        "            df_zip_county['COUNTY_FIPS'] = df_zip_county['COUNTY_FIPS'].str.zfill(5)\n",
        "            df_zip_county = df_zip_county.dropna().drop_duplicates(subset=['ZIP'])\n",
        "            print(f\"Loaded crosswalk for {len(df_zip_county)} ZIP codes\")\n",
        "        else:\n",
        "            print(\"crosswalk data file not found\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading crosswalk data: {e}\")\n",
        "        return None\n",
        "\n",
        "    return df_housing, df_nri, df_zip_county\n",
        "\n",
        "def calculate_housing_price_changes(df_merged):\n",
        "    \"\"\"calc housing price changes based on climate risk\"\"\"\n",
        "\n",
        "    print(f\"Climate risk range: {df_merged['climate_risk'].min():.2f} to {df_merged['climate_risk'].max():.2f}\")\n",
        "    print(f\"Climate risk mean: {df_merged['climate_risk'].mean():.2f}\")\n",
        "\n",
        "    risk_max = df_merged['climate_risk'].max()\n",
        "\n",
        "    if risk_max <= 100:\n",
        "        # Using RISK_SCORE (0-100 scale)\n",
        "        # Map risk score to housing price changes\n",
        "        # High risk areas see price decreases due to climate concerns\n",
        "        # 0-20: +2% to +1% (low risk areas appreciate)\n",
        "        # 20-40: +1% to 0% (below average risk)\n",
        "        # 40-60: 0% to -2% (average risk)\n",
        "        # 60-80: -2% to -5% (above average risk)\n",
        "        # 80-100: -5% to -8% (high risk areas depreciate)\n",
        "\n",
        "        risk_score = df_merged['climate_risk']\n",
        "        df_merged['price_change_pct'] = np.where(\n",
        "            risk_score <= 20, 2 - (risk_score / 20) * 1,  # +2% to +1%\n",
        "            np.where(\n",
        "                risk_score <= 40, 1 - ((risk_score - 20) / 20) * 1,  # +1% to 0%\n",
        "                np.where(\n",
        "                    risk_score <= 60, 0 - ((risk_score - 40) / 20) * 2,  # 0% to -2%\n",
        "                    np.where(\n",
        "                        risk_score <= 80, -2 - ((risk_score - 60) / 20) * 3,  # -2% to -5%\n",
        "                        -5 - ((risk_score - 80) / 20) * 3  # -5% to -8%\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        print(\"Using RISK_VALUE (dollar amounts)\")\n",
        "        risk_mean = df_merged['climate_risk'].mean()\n",
        "        risk_std = df_merged['climate_risk'].std()\n",
        "        df_merged['risk_zscore'] = (df_merged['climate_risk'] - risk_mean) / risk_std\n",
        "\n",
        "        # map z-scores to housing price changes (inverse)\n",
        "        # higher risk = lower price appreciation/depreciation\n",
        "        df_merged['price_change_pct'] = -df_merged['risk_zscore'] * 3  # 3% per SD\n",
        "\n",
        "    # bounds\n",
        "    df_merged['price_change_pct'] = np.clip(df_merged['price_change_pct'], -8, 3)\n",
        "\n",
        "    # calc projected changes\n",
        "    df_merged['projected_home_value_2025'] = df_merged['Current_Value'] * (\n",
        "        1 + df_merged['price_change_pct'] / 100\n",
        "    )\n",
        "\n",
        "    df_merged['price_change_dollars'] = (\n",
        "        df_merged['projected_home_value_2025'] - df_merged['Current_Value']\n",
        "    )\n",
        "\n",
        "    return df_merged\n",
        "\n",
        "def create_housing_price_table():\n",
        "    \"\"\"Main function to create housing price change analysis table\"\"\"\n",
        "    print(\"Starting housing price change analysis...\")\n",
        "\n",
        "    # load data\n",
        "    df_housing, df_nri, df_zip_county = load_housing_data_with_fallback()\n",
        "\n",
        "    # merge housing -> crosswalk -> risk\n",
        "    df_merged = df_housing.merge(df_zip_county, left_on='ZIP_Code', right_on='ZIP', how='inner')\n",
        "\n",
        "    df_merged = df_merged.merge(df_nri, left_on='COUNTY_FIPS', right_on='COUNTYFIPS', how='inner')\n",
        "\n",
        "    # calc price changes\n",
        "    df_result = calculate_housing_price_changes(df_merged)\n",
        "\n",
        "    # summary table\n",
        "    results_table = df_result[[\n",
        "        'ZIP_Code', 'Location', 'Current_Value', 'climate_risk',\n",
        "        'price_change_pct', 'price_change_dollars', 'projected_home_value_2025'\n",
        "    ]].copy()\n",
        "\n",
        "    def categorize_risk(risk_value, risk_max):\n",
        "        if risk_max <= 100:\n",
        "            if risk_value <= 20:\n",
        "                return \"Very Low\"\n",
        "            elif risk_value <= 40:\n",
        "                return \"Low\"\n",
        "            elif risk_value <= 60:\n",
        "                return \"Moderate\"\n",
        "            elif risk_value <= 80:\n",
        "                return \"High\"\n",
        "            else:\n",
        "                return \"Very High\"\n",
        "        else:\n",
        "            percentiles = np.percentile(df_result['climate_risk'], [20, 40, 60, 80])\n",
        "            if risk_value <= percentiles[0]:\n",
        "                return \"Very Low\"\n",
        "            elif risk_value <= percentiles[1]:\n",
        "                return \"Low\"\n",
        "            elif risk_value <= percentiles[2]:\n",
        "                return \"Moderate\"\n",
        "            elif risk_value <= percentiles[3]:\n",
        "                return \"High\"\n",
        "            else:\n",
        "                return \"Very High\"\n",
        "\n",
        "    risk_max = results_table['climate_risk'].max()\n",
        "    results_table['Risk Category'] = results_table['climate_risk'].apply(\n",
        "        lambda x: categorize_risk(x, risk_max)\n",
        "    )\n",
        "\n",
        "    # sort by price change (most negative first)\n",
        "    results_table = results_table.sort_values('price_change_pct')\n",
        "\n",
        "    results_table['Current Value'] = results_table['Current_Value'].apply(lambda x: f\"${x:,.0f}\")\n",
        "    results_table['Price Change'] = results_table['price_change_pct'].apply(lambda x: f\"{x:+.1f}%\")\n",
        "    results_table['Dollar Change'] = results_table['price_change_dollars'].apply(lambda x: f\"${x:+,.0f}\")\n",
        "    results_table['Projected 2025 Value'] = results_table['projected_home_value_2025'].apply(lambda x: f\"${x:,.0f}\")\n",
        "    results_table['Climate Risk Score'] = results_table['climate_risk'].apply(lambda x: f\"{x:.1f}\")\n",
        "\n",
        "    # final table\n",
        "    display_table = results_table[[\n",
        "        'ZIP_Code', 'Location', 'Risk Category', 'Climate Risk Score',\n",
        "        'Current Value', 'Price Change', 'Dollar Change', 'Projected 2025 Value'\n",
        "    ]].copy()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*120)\n",
        "    print(\"HOUSING PRICE CHANGE ANALYSIS - CLIMATE RISK IMPACT\")\n",
        "    print(\"=\"*120)\n",
        "\n",
        "    # print the table\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', None)\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "    print(display_table.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*120)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\"*120)\n",
        "\n",
        "    # summary stats\n",
        "    total_zips = len(results_table)\n",
        "    avg_change_pct = results_table['price_change_pct'].mean()\n",
        "    avg_change_dollars = results_table['price_change_dollars'].mean()\n",
        "\n",
        "    declining_zips = len(results_table[results_table['price_change_pct'] < 0])\n",
        "    appreciating_zips = len(results_table[results_table['price_change_pct'] > 0])\n",
        "\n",
        "    print(f\"Total ZIP codes analyzed: {total_zips}\")\n",
        "    print(f\"Average price change: {avg_change_pct:+.2f}%\")\n",
        "    print(f\"Average dollar change: ${avg_change_dollars:+,.0f}\")\n",
        "    print(f\"ZIP codes with declining values: {declining_zips} ({declining_zips/total_zips*100:.1f}%)\")\n",
        "    print(f\"ZIP codes with appreciating values: {appreciating_zips} ({appreciating_zips/total_zips*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nBy Risk Category:\")\n",
        "    risk_summary = results_table.groupby('Risk Category').agg({\n",
        "        'price_change_pct': 'mean',\n",
        "        'price_change_dollars': 'mean',\n",
        "        'ZIP Code': 'count'\n",
        "    }).round(2)\n",
        "\n",
        "    for risk_cat, row in risk_summary.iterrows():\n",
        "        print(f\"  {risk_cat}: {row['ZIP Code']} ZIPs, avg change: {row['price_change_pct']:+.1f}% (${row['price_change_dollars']:+,.0f})\")\n",
        "\n",
        "    most_impacted = results_table.loc[results_table['price_change_pct'].idxmin()]\n",
        "    least_impacted = results_table.loc[results_table['price_change_pct'].idxmax()]\n",
        "\n",
        "    print(f\"\\nMost Impacted Area:\")\n",
        "    print(f\"  {most_impacted['Location']} ({most_impacted['ZIP Code']}): {most_impacted['Price Change']} (${most_impacted['price_change_dollars']:+,.0f})\")\n",
        "\n",
        "    print(f\"\\nLeast Impacted Area:\")\n",
        "    print(f\"  {least_impacted['Location']} ({least_impacted['ZIP Code']}): {least_impacted['Price Change']} (${least_impacted['price_change_dollars']:+,.0f})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*120)\n",
        "    print(\"NOTE: Analysis based on climate risk data. Higher risk areas may see price declines\")\n",
        "    print(\"due to increased insurance costs, infrastructure concerns, and buyer preferences.\")\n",
        "    print(\"=\"*120)\n",
        "\n",
        "    return display_table, results_table\n",
        "\n",
        "# run analysis\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        display_table, full_results = create_housing_price_table()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IU9OOZa4EQOD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}